{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNTXY9ldokIm"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load & pre-process MNIST dataset\n",
        "\n",
        "def load_data():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0  # normalize to [0, 1]\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# convert labels to one-hot encoding\n",
        "def one_hot_encode(labels, num_classes=10):\n",
        "    return np.eye(num_classes)[labels]"
      ],
      "metadata": {
        "id": "Mo1NQv3To1wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility functions\n",
        "\n",
        "# sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -500, 500)  # clamp to prevent overflow\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# sigmoid derivative\n",
        "def sigmoid_derivative(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "# softmax activation function\n",
        "def softmax(x):\n",
        "    if x.ndim == 1:  # for 1D arrays\n",
        "        exps = np.exp(x - np.max(x))\n",
        "        return exps / np.sum(exps)\n",
        "    elif x.ndim == 2:  # for 2D arrays\n",
        "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "# forward pass for convolution\n",
        "def conv2d(input, kernel):\n",
        "    h, w = input.shape\n",
        "    kh, kw = kernel.shape\n",
        "    output = np.zeros((h - kh + 1, w - kw + 1))\n",
        "    for i in range(output.shape[0]):\n",
        "        for j in range(output.shape[1]):\n",
        "            output[i, j] = np.sum(input[i:i+kh, j:j+kw] * kernel)\n",
        "    return output\n",
        "\n",
        "# average pooling\n",
        "def avg_pooling(input, pool_size, stride):\n",
        "    h, w = input.shape\n",
        "    ph, pw = pool_size\n",
        "    output = []\n",
        "    for i in range(0, h - ph + 1, stride):\n",
        "        row = []\n",
        "        for j in range(0, w - pw + 1, stride):\n",
        "            region = input[i:i+ph, j:j+pw]\n",
        "            row.append(np.mean(region))\n",
        "        output.append(row)\n",
        "    return np.array(output)\n",
        "\n",
        "# flatten function\n",
        "def flatten(input):\n",
        "    return input.flatten()\n",
        "\n",
        "# cross-entropy loss\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    n = y_pred.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-15)) / n"
      ],
      "metadata": {
        "id": "jUfupcfcpQ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement CNN architecture\n",
        "\n",
        "# initialize weights for CNN\n",
        "def initialize_weights():\n",
        "    np.random.seed(0)\n",
        "    conv_kernel = np.random.randn(2, 3, 3) * 0.1  # two kernels of size 3x3\n",
        "    fc_weights = np.random.randn(10, 2 * 13 * 13) * 0.1  # fully connected weights\n",
        "    return conv_kernel, fc_weights\n",
        "\n",
        "# forward propagation\n",
        "def forward_propagation(x, conv_kernel, fc_weights):\n",
        "    conv_outputs = [sigmoid(conv2d(x, kernel)) for kernel in conv_kernel]\n",
        "    pooled_outputs = [avg_pooling(output, (2, 2), 2) for output in conv_outputs]\n",
        "    flattened = np.concatenate([output.flatten() for output in pooled_outputs])  # concatenate flattened pooled outputs\n",
        "    logits = np.dot(fc_weights, flattened)  # fully connected layer\n",
        "    predictions = softmax(logits)  # apply softmax to logits\n",
        "    return predictions, conv_outputs, pooled_outputs, flattened, logits\n",
        "\n",
        "# backward propagation\n",
        "def backward_propagation(x, y_true, conv_outputs, pooled_outputs, flattened, logits, conv_kernel, fc_weights, lr=0.01):\n",
        "    # gradient of softmax + cross-entropy\n",
        "    softmax_grad = softmax(logits) - y_true\n",
        "\n",
        "    # Gradients for fully connected layer\n",
        "    fc_grad = np.outer(softmax_grad, flattened)\n",
        "\n",
        "    # gradients for convolutional kernels\n",
        "    kernel_grads = np.zeros_like(conv_kernel)\n",
        "    for i, kernel in enumerate(conv_kernel):\n",
        "        pooled_grad = sigmoid_derivative(conv_outputs[i])  # backprop through pooling & activation\n",
        "        kernel_grads[i] = conv2d(x, pooled_grad)\n",
        "\n",
        "    # update weights\n",
        "    fc_weights -= lr * fc_grad\n",
        "    conv_kernel -= lr * kernel_grads\n",
        "\n",
        "# training loop\n",
        "def train(x_train, y_train, conv_kernel, fc_weights, epochs=10, lr=0.01):\n",
        "    for epoch in range(epochs):\n",
        "        loss = 0\n",
        "        for i in range(len(x_train)):\n",
        "            x = x_train[i]\n",
        "            y = y_train[i]\n",
        "            y_true = one_hot_encode(np.array([y]))\n",
        "            predictions, conv_outputs, pooled_outputs, flattened, logits = forward_propagation(x, conv_kernel, fc_weights)\n",
        "            loss += cross_entropy_loss(predictions, y_true)\n",
        "            backward_propagation(x, y_true, conv_outputs, pooled_outputs, flattened, logits, conv_kernel, fc_weights, lr)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss / len(x_train)}\")\n",
        "\n",
        "# evaluation function\n",
        "def evaluate(x_test, y_test, conv_kernel, fc_weights):\n",
        "    correct = 0\n",
        "    for i in range(len(x_test)):\n",
        "        x = x_test[i]\n",
        "        y = y_test[i]\n",
        "        predictions, *_ = forward_propagation(x, conv_kernel, fc_weights)\n",
        "        if np.argmax(predictions) == y:\n",
        "            correct += 1\n",
        "    accuracy = correct / len(x_test)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "naWPG_TXo-dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train & view results of CNN\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_data()\n",
        "y_train_one_hot = one_hot_encode(y_train)\n",
        "\n",
        "conv_kernel, fc_weights = initialize_weights()\n",
        "\n",
        "train(x_train, y_train, conv_kernel, fc_weights, epochs=10, lr=0.01)\n",
        "evaluate(x_test, y_test, conv_kernel, fc_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f5X9jP3p450",
        "outputId": "96b557f8-dfb2-4f46-b61c-b15c8662988e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/10, Loss: 0.05028540449492675\n",
            "Epoch 2/10, Loss: 0.038176612262655424\n",
            "Epoch 3/10, Loss: 0.03637649503355718\n",
            "Epoch 4/10, Loss: 0.03547008456537964\n",
            "Epoch 5/10, Loss: 0.03490383922425014\n",
            "Epoch 6/10, Loss: 0.034509492658570236\n",
            "Epoch 7/10, Loss: 0.03421602293758765\n",
            "Epoch 8/10, Loss: 0.0339876292365444\n",
            "Epoch 9/10, Loss: 0.033804064069005886\n",
            "Epoch 10/10, Loss: 0.03365290203624081\n",
            "Accuracy: 88.19%\n"
          ]
        }
      ]
    }
  ]
}